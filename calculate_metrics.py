import os
import torch
import numpy as np
from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, average_precision_score
import re

# === é…ç½®è·¯å¾„ ===
LOG_DIR = r"D:\AML\Visual-Place-Recognition-Project\logs\log_dir\2025-12-07_19-13-12"
Z_DATA_PATH = os.path.join(LOG_DIR, "z_data.torch")
INLIERS_DIR = os.path.join(LOG_DIR, "preds_superpoint-lg")

def numerical_sort_key(filename):
    numbers = re.findall(r'\d+', filename)
    return int(numbers[0]) if numbers else filename

def calculate_metrics():
    print("ğŸš€ Loading Data...")
    
    # 1. åŠ è½½ Ground Truth å’Œ æ£€ç´¢é¢„æµ‹
    if not os.path.exists(Z_DATA_PATH):
        print("âŒ Error: z_data.torch not found.")
        return

    z_data = torch.load(Z_DATA_PATH, weights_only=False)
    predictions = z_data['predictions']          # Shape: [Num_Queries, Top_K]
    positives = z_data['positives_per_query']    # Ground Truth indices
    
    # === PART 1: è®¡ç®—æ ‡å‡† VPR Recall@N ===
    print("\nğŸ“Š Calculating Standard VPR Recall@N...")
    
    recalls = {1: [], 5: [], 10: [], 20: []}
    
    # éå†æ¯ä¸ªæŸ¥è¯¢
    for i in range(len(predictions)):
        # è·å–è¯¥æŸ¥è¯¢çš„ Top-K é¢„æµ‹ (é€šå¸¸ K=20)
        preds = predictions[i]
        if isinstance(preds, torch.Tensor):
            preds = preds.tolist()
            
        true_matches = positives[i] # è¯¥æŸ¥è¯¢å¯¹åº”çš„çœŸå®æ­£ç¡®ç´¢å¼•åˆ—è¡¨
        
        # æ£€æŸ¥ Top-N æ˜¯å¦å‘½ä¸­
        for n in recalls.keys():
            # å–å‰ N ä¸ªé¢„æµ‹
            top_n = preds[:n]
            # åˆ¤æ–­æ˜¯å¦æœ‰ä»»æ„ä¸€ä¸ªåœ¨ true_matches é‡Œ
            hit = any(p in true_matches for p in top_n)
            recalls[n].append(1 if hit else 0)

    # è¾“å‡º Recall ç»“æœ
    print("-" * 30)
    for n in sorted(recalls.keys()):
        avg_recall = np.mean(recalls[n]) * 100
        print(f"Recall@{n}: {avg_recall:.2f}%")
    print("-" * 30)

    # === PART 2: è®¡ç®—ä¸ç¡®å®šæ€§æŒ‡æ ‡ (AUPRC/AUROC) ===
    # è¿™ä¸€æ­¥è¯„ä¼°â€œå†…ç‚¹æ•°â€æ˜¯å¦æ˜¯ä¸€ä¸ªå¥½çš„ç½®ä¿¡åº¦æŒ‡æ ‡
    print("\nğŸ“‰ Calculating Uncertainty Metrics (based on Inliers)...")
    
    # 1. è¯»å–å†…ç‚¹æ•° (ä½œä¸º Score)
    files = sorted([f for f in os.listdir(INLIERS_DIR) if f.endswith(".torch")], key=numerical_sort_key)
    
    # ç¡®ä¿æ•°æ®å¯¹é½
    min_len = min(len(files), len(predictions))
    files = files[:min_len]
    binary_labels = recalls[1][:min_len] # ä½¿ç”¨ R@1 çš„ç»“æœä½œä¸ºæ ‡ç­¾ (1=Correct, 0=Wrong)
    
    inlier_scores = []
    
    for filename in files:
        try:
            data = torch.load(os.path.join(INLIERS_DIR, filename), weights_only=False)
            # è·å–æœ€å¤§å†…ç‚¹æ•°ä½œä¸ºè¯¥æŸ¥è¯¢çš„ç½®ä¿¡åº¦
            if isinstance(data, list):
                counts = [x['num_inliers'] for x in data if isinstance(x, dict) and 'num_inliers' in x]
                score = max(counts) if counts else 0
            else:
                score = 0
            inlier_scores.append(score)
        except:
            inlier_scores.append(0)

    # 2. è®¡ç®—æŒ‡æ ‡
    # æ³¨æ„ï¼šAUPRC éœ€è¦ inputs æ˜¯ numpy array
    y_true = np.array(binary_labels)
    y_scores = np.array(inlier_scores)
    
    if len(y_true) > 0:
        # AUPRC (Average Precision)
        auprc = average_precision_score(y_true, y_scores)
        
        # AUROC
        try:
            auroc = roc_auc_score(y_true, y_scores)
        except:
            auroc = 0.5 # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼ŒAUROC æ— æ³•è®¡ç®—

        print(f"AUPRC (Average Precision): {auprc:.4f} (Higher is better)")
        print(f"AUROC: {auroc:.4f}")
        print("Interpretation: High AUPRC means Inliers are a good predictor of correctness.")
    else:
        print("âš ï¸ Not enough data to calculate AUPRC.")

if __name__ == "__main__":
    calculate_metrics()